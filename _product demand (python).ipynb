#!/usr/bin/env python
# coding: utf-8

# In[1]:


#downloading dataset, importing libraries
import pandas as pd

import numpy as np

import os

import matplotlib.pyplot as plt

import seaborn as sns

import sys

from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import OrdinalEncoder
from sklearn.model_selection import train_test_split 
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error
from sklearn import tree
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor


# In[2]:


#reading the dataset, deleting missing value entries
df = pd.read_csv('Historical Product Demand.csv')

df.head()

df.dropna()


# In[3]:


#displaying actual column names
df.columns


# In[4]:


# encoding "product_code, warehouse and product_category columns; and selecting features
enc = OrdinalEncoder()


df[['Product_Code', 'Warehouse', 'Product_Category']] = enc.fit_transform(df[['Product_Code', 'Warehouse', 'Product_Category']])

# date column not selected
selected_features = df[['Product_Code', 'Warehouse', 'Product_Category']]
display(selected_features)


# In[5]:


# removing commas and spaces to prevent errors
df['Order_Demand'] = df['Order_Demand'].str.replace("(","")

df['Order_Demand'] = df['Order_Demand'].str.replace(")","")


# In[6]:


#selecting and scaling X variable (selected features).
#converting required columns into numberic
df[['Product_Code', 'Warehouse', 'Product_Category', 'Order_Demand']] = df[['Product_Code', 'Warehouse', 'Product_Category','Order_Demand']].apply(pd.to_numeric)
X = selected_features


# In[7]:


# defining y variable

y = df['Order_Demand'].values


# In[8]:


#creating a split (train/test)
X_train, X_test_val, y_train, y_test_val = train_test_split(X, y, test_size=0.2, random_state=45)


#creating a split (train/val)

X_test, X_val, y_test, y_val = train_test_split(X_test_val, y_test_val, test_size=0.5, random_state=45)

# now the train/validate/test split will be 80%/10%/10%


# In[9]:


#decision tree regressor (part 1):
#squared_error criterion

DTR = DecisionTreeRegressor(criterion = 'squared_error')
DTR = DTR.fit(X_train,y_train)
y_pred = DTR.predict(X_test)
score = DTR.score(X_test,y_test)
print ("Squared Error", DTR.score(X_test, y_test))
    


# In[10]:


#decision tree regressor (part 2):
#poisson criterion
DTR1 = DecisionTreeRegressor(criterion = 'poisson')
DTR1 = DTR1.fit(X_train,y_train)
y_pred1 = DTR1.predict(X_test)
score1 = DTR1.score(X_test,y_test)
print ("Poisson", DTR1.score(X_test, y_test))


# In[11]:


#decision tree regressor (part 3):
#friedman_mse criterion
DTR2 = DecisionTreeRegressor(criterion = 'friedman_mse')
DTR2 = DTR2.fit(X_train,y_train)
y_pred2 = DTR2.predict(X_test)
score2 = DTR2.score(X_test,y_test)
print ("Friedman MSe", DTR2.score(X_test, y_test))


# In[12]:


#training SVR Regressor model withmax iterations of 100

for k in ['linear', 'poly', 'rbf', 'sigmoid']:
    dat = SVR(kernel = k, max_iter = 100)
    dat.fit(X_train, y_train)
    scores = dat.score(X_test, y_test)
    print (k, scores)


# In[13]:


#training KNeighbors Regressor (step 6)
KNN = KNeighborsRegressor(n_neighbors =1000)
KNN.fit(X_train, y_train)
y_pred = KNN.predict(X_test)
scoreKNN = KNN.score(X_test, y_test)
print (scoreKNN)
#print(mean_squared_error(y_test, y_pred))


# In[14]:


# conclusion
print ("All the 4 criterion of the Decision Tree Regressor give the same R squared score")
print ("All 4 kernels for SVR Regressor,except for 'sigmoid'; give same R squared score")
print ("out of 3 Regressor models tried above; Decision Tree Regressor gives best R squared score")

